{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "# Add parent folder to syspath to include local util functions\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "from utils.plot_utils import plot_hist\n",
    "from preprocess_utils.preprocessing_utils import apply_yeojohnson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../../data/\"\n",
    "plots_dir = \"../../plots/\"\n",
    "\n",
    "# Reload data that was processed for a bit before:\n",
    "df = pd.read_csv(data_dir + \"investigated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pandas_profile Report\n",
    "profile = ProfileReport(df, minimal=True)\n",
    "profile.to_widgets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the report to disk\n",
    "profile.to_file(plots_dir + \"profile_report.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the feature distributions\n",
    "plot_hist(df, plots_dir=plots_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix IL6 and SORL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"T1_SORL1\"] == 0, \"T1_SORL1\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"T1_IL6\"] == 0, \"T1_IL6\"] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = apply_yeojohnson(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms:\n",
    "plot_hist(df, name=\"Yeojohnsoned\", plots_dir=plots_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weirdos = df.loc[:, (df.columns.str.contains(\"MissingRepl|Unreife|Troponin\"))]\n",
    "plot_hist(weirdos, name=\"weirdos\", plots_dir=plots_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[:, df.columns.str.contains(\"[^T1_Troponin$T1_NTproBNP$T1_MDA$T1_Leptin_Lab$]\")]\n",
    "plot_hist(df, name=\"without dropped params\", plots_dir=plots_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df, name, quantile=0.99):\n",
    "    print(name)\n",
    "    q95m = df[df[\"sex\"] == 0][name].quantile(quantile)\n",
    "    q95f = df[df[\"sex\"] == 1][name].quantile(quantile)\n",
    "    print(df[name].describe())\n",
    "    print(f\"Male: {q95m}\")\n",
    "    print(f\"Female: {q95f}\")\n",
    "    # replace outliers with 95 quantile cut-off value for respective sex\n",
    "    df.loc[(df[\"sex\"] == 0) & (df[name] > q95m), name] = q95m\n",
    "    df.loc[(df[\"sex\"] == 1) & (df[name] > q95f), name] = q95f\n",
    "    # check the result\n",
    "    print(\"Std now: \", df[name].std())\n",
    "    print()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "fragile_params = ['T1_oxLDL', 'T1_S100A12_plasma',\n",
    "       'T1_Calprotectinn', 'T1_KNYAcid', 'T1_NTproBNP',\n",
    "       'T1_NTproBNP_MissingRepl', 'T1_CRP_InclExtrapol', 'T1_S100A12',\n",
    "       'T1_ALAT_GPT_U_L', 'T1_ASAT_GOT_U_L', 'T1_gammaGTSe', \"T1_Triglycerides_mmolL\",\n",
    "       'T1_Triglyc_mmolL_Reanalysis', 'T1_UnreifeGranulozytenabsolut', 'T1_UnreifeGranulozyten_Percent',\n",
    "       'T1_IL2_pgml', 'T1_IL6', 'T1_Cpeptide_total', 'T1_Leptin_total',\n",
    "       'T1_Leptin_SLR_Ratio', 'T1_SORL1', 'T1_IL18_pgml',\n",
    "       'Final_T1_TP42_40', \"T1_FinalTG_mmolL\"]\n",
    "for param in df.columns:\n",
    "    df = remove_outliers(df, param, quantile=0.9999)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normal range T1_ASAT_GOT_U_L: 35 m / 30 f (<=)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def normalize(df):\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(df)\n",
    "\n",
    "    # Apply normalization:\n",
    "    df_normalized = scaler.transform(df)\n",
    "    df = pd.DataFrame(df_normalized, columns=df.columns)\n",
    "    return df\n",
    "\n",
    "# Save means and stds:\n",
    "#np.save(\"data/means\", df_means.to_numpy())\n",
    "#np.save(\"data/stds\", df_stds.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normalized = normalize(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(df_normalized, columns=df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified Train-test-val split and k-fold split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labels(df):\n",
    "    df_minuses = df.fillna(-1)\n",
    "    #df_minuses.loc[df[\"PreCI_dichotomous_T0\"] == -1, \"PreCI_dichotomous_T0\"] = 0\n",
    "    mean_age = df[\"Alter\"].mean()\n",
    "    return [(df_minuses.iloc[idx][\"Alter\"] < mean_age).astype(int).astype(str) +\n",
    "            df_minuses.iloc[idx][\"sex\"].astype(int).astype(str) +\n",
    "            df_minuses.iloc[idx][\"POD\"].astype(int).astype(str) +\n",
    "            df_minuses.iloc[idx][\"POCD_dichotomous_T2\"].astype(int).astype(str) #+\n",
    "            #df_minuses.iloc[idx][\"PreCI_dichotomous_T0\"].astype(int).astype(str)\n",
    "            #PreCI_dichotomous_T0\n",
    "            for idx in range(len(df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_balanced_split(df, test_size, hard_threshold, soft_threshold, num_allowed):\n",
    "    count = 0\n",
    "    names = np.array(df.columns)\n",
    "    print(\"df mean: \", df.mean().mean())\n",
    "    outliers = num_allowed + 1\n",
    "    max_diff = hard_threshold + 1\n",
    "    while outliers > num_allowed or max_diff > hard_threshold:\n",
    "        # Create split:\n",
    "        indices = np.arange(len(df))\n",
    "        labels = create_labels(df)\n",
    "        train_data, test_data, train_idcs, test_idcs = train_test_split(df, indices, test_size=test_size, stratify=labels)\n",
    "        # Test if split is good enough:\n",
    "        print(test_data.shape)\n",
    "        diffs = np.array([0])#np.abs(test_data.mean(axis=0) - train_data.mean(axis=0)) / np.abs(train_data.mean(axis=0))\n",
    "        max_diff = max(diffs)\n",
    "        #print(\"first: \", np.abs(test_data.mean(axis=0) - train_data.mean(axis=0)))\n",
    "        print(test_data.mean().mean(), train_data.mean().mean(), df.mean().mean())\n",
    "        print(list(np.round(diffs[diffs > soft_threshold], 2)))\n",
    "        #print(names[diffs > soft_threshold])\n",
    "        print(\"Mean train data: \", train_data.mean(), \"Mean test data: \", test_data.mean())\n",
    "        print(\"Mean deviation: \", np.mean(diffs), \"Max deviation:\", max_diff)\n",
    "        outliers = (diffs > soft_threshold).sum()\n",
    "        count += 1\n",
    "        if count == 100:\n",
    "            raise StopIteration(\"Can't find balanced split\")\n",
    "        print(\"Num outliers: \", outliers)\n",
    "        print()\n",
    "    return train_idcs, test_idcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def split_df(df, test_size, val_size, **kwargs):\n",
    "    print(\"Test split:\")\n",
    "    train_idcs, _ = create_balanced_split(df, test_size, **kwargs)\n",
    "    print(\"Validation split:\")\n",
    "    _, val_idcs = create_balanced_split(df.iloc[train_idcs], test_size, **kwargs)\n",
    "    return train_idcs, val_idcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "def create_k_fold(train_data, k):\n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=True)\n",
    "    train_labels = create_labels(train_data)\n",
    "    splits = skf.split(train_data, train_labels)\n",
    "    splits = [split[1] for split in splits]\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and save train-test indices:\n",
    "train_idcs, val_idcs = split_df(df, test_size=0.2, val_size=0.2, hard_threshold=0.15, soft_threshold=0.1, num_allowed=2)\n",
    "np.save(data_dir + \"/train_idcs\", train_idcs)\n",
    "np.save(data_dir + \"/val_idcs\", val_idcs)\n",
    "\n",
    "# Create and save k-fold indices:\n",
    "k = 5\n",
    "splits = create_k_fold(df.iloc[train_idcs], 5)\n",
    "split_path = data_dir + \"/\" + str(k) + \"_folds/\"\n",
    "os.makedirs(split_path, exist_ok=True)\n",
    "for idx, split in enumerate(splits):\n",
    "    np.save(split_path + str(idx), split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_df(df, name):\n",
    "    \"\"\"Stores a fully processed df (filled NANs etc.)\"\"\"\n",
    "    path = data_dir + name + \"/\"\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    # Extract and store outcomes:\n",
    "    POD = df[\"POD\"].to_numpy()\n",
    "    POCD = df[\"POCD_dichotomous_T2\"].to_numpy()\n",
    "    np.save(path + \"POD\", POD)\n",
    "    np.save(path + \"POCD\", POCD)\n",
    "    df_no_outcomes = df.drop(columns=[\"POD\", \"POCD_dichotomous_T2\"])\n",
    "    # Extract inputs separately:\n",
    "    blood_names = [col for col in df_no_outcomes.columns if \"T1_\" in col]\n",
    "    blood_vals = df_no_outcomes[blood_names].to_numpy()\n",
    "    static_names = [col for col in df_no_outcomes.columns if \"T1_\" not in col]\n",
    "    static_vals = df_no_outcomes[static_names].to_numpy()\n",
    "    #print(blood_names)\n",
    "    #print(static_names)\n",
    "    np.save(path + \"blood_names\", blood_names)\n",
    "    np.save(path + \"blood_vals\", blood_vals)\n",
    "    np.save(path + \"static_names\", static_names)\n",
    "    np.save(path + \"static_vals\", static_vals)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing target values as minus ones:\n",
    "df[[\"POD\", \"POCD_dichotomous_T2\"]] = df[[\"POD\", \"POCD_dichotomous_T2\"]].fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and store differently filled dfs:\n",
    "# Mean imputation:\n",
    "df_means = df.mean(axis=0)\n",
    "df_mean_filled = df.copy()\n",
    "df_mean_filled[\"PreCI_dichotomous_T0\"].fillna(df[\"PreCI_dichotomous_T0\"].mode()[0], inplace=True)\n",
    "print(df_mean_filled[\"PreCI_dichotomous_T0\"].unique())\n",
    "df_mean_filled = df_mean_filled.fillna(df_means)\n",
    "store_df(df_mean_filled, \"data_mean_filled\")\n",
    "# Median imputation:\n",
    "df_mean_filled = df.fillna(df.median())\n",
    "store_df(df_mean_filled, \"data_median_filled\")\n",
    "# Minuse one imputation:\n",
    "df_minuses = df.fillna(-1)\n",
    "store_df(df_minuses, \"data_minus_filled\")\n",
    "# IterativeImputer:\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import IterativeImputer\n",
    "imputer = IterativeImputer()#estimator=None, missing_values=np.nan, sample_posterior=False, \n",
    "                           #max_iter=10, tol=0.001, n_nearest_features=None, initial_strategy='mean', \n",
    "                           #imputation_order='ascending', skip_complete=False, min_value=None, \n",
    "                           #max_value=None, verbose=0, random_state=None, add_indicator=False)\n",
    "#imputer.fit(df.to_numpy())\n",
    "#df_imputed = df.transform(df)\n",
    "#store_df(df_imputed, \"IterativeImputed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot and save cleaned histograms\n",
    "plot_hist(df, name=\"Cleaned\", plots_dir=plots_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
