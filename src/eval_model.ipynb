{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import optuna\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('../..'))\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "from src.train import _init_model, start_training, get_args\n",
    "from src.utils.load_data_utils import get_data, get_train_eval_data\n",
    "import src.rfe as rfe\n",
    "from src.eval import evaluate_model\n",
    "from src.tune import get_tune_args\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load best model params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Specify this: ########################\n",
    "study_name = \"Nov-14-21:23:35_xgb_prauc_3_clinical_20000\"\n",
    "####################################################\n",
    "\n",
    "def get_best_trial(study_name):\n",
    "    best_args = ''\n",
    "    study_folder = os.path.join(\"optuna_studies\", study_name)\n",
    "    for file in os.listdir(study_folder):\n",
    "        if file.endswith(\"rank2.pkl\"):\n",
    "            best_args = joblib.load(os.path.join(study_folder, file))\n",
    "    return best_args\n",
    "\n",
    "best_args = get_best_trial(study_name)\n",
    "best_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.study.load_study(study_name, storage=\"sqlite:///optuna.db\")\n",
    "study_sorted = study.trials_dataframe().sort_values('value', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_trial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Retrain model with best hyperparams for further evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use dev set \n",
    "best_args.nf = 0\n",
    "best_args.split = \"dev/test\" \n",
    "\n",
    "# save retrained model\n",
    "# best_args.save = True\n",
    "\n",
    "# load data\n",
    "x_train, y_train, x_eval, y_eval, _, feature_names, class_weights = get_data(best_args) \n",
    "\n",
    "# retrain with best hyperparams\n",
    "_, models = start_training(best_args, None, 'auc', x_train, y_train, x_eval, y_eval, feature_names, class_weights)\n",
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluate retrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (optional) If you want to load a specific model from the `models` directory:\n",
    "... else uses the above returned models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Specify this (optional): ########################\n",
    "model_name = \"\"\n",
    "####################################################\n",
    "\n",
    "if model_name != \"\":\n",
    "    model_folder = os.path.abspath('../../models')\n",
    "    model_path = os.path.join(model_folder, model_name)\n",
    "    models = [np.load(model_path)]\n",
    "    print(\"Loading `model` from directory\")\n",
    "else:\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run evaluation (pre RFE)\n",
    "Works for both single and multiple models / folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use test split\n",
    "test_args = best_args\n",
    "test_args.split = \"dev/test\" # will be used only if nf set to 0\n",
    "test_args.nf = 0\n",
    "_, _, x_test, y_test, _, feature_names, _ = get_data(test_args) \n",
    "\n",
    "# define output directory for eval results\n",
    "test_output_dir = os.path.join(\"../final_eval_results\", study_name, \"pre-rfe\")\n",
    "os.makedirs(test_output_dir, exist_ok=True)\n",
    "\n",
    "# run evaluation (computs cumulative metrics if more than one model is evaluated here)\n",
    "evaluate_model(models, x_test, y_test, test_output_dir, feature_names, test_args)\n",
    "\n",
    "#print(f\"Find evaluation results in\\n {test_output_dir}.\")\n",
    "#study = optuna.study.load_study(study_name, storage=\"sqlite:///optuna.db\")\n",
    "#print(f\"\\noptuna value : {study.best_trial.value:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.read_csv(\"/home/angelie/Documents/AdaLab/pharmaimage/final_eval_results/Nov-14-21:25:01_xgb_prauc_3_clinical,blood_20000/pre-rfe/XGBClassifier_feature_importances.csv\")\n",
    "feature_importances = feature_importances.sort_values(0, axis=1, ascending=False)\n",
    "feature_importances[feature_importances == 0] = np.nan\n",
    "feature_importances = feature_importances.dropna(axis=1)\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances.loc[:, :'blood_T1_SORL1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances.loc[0, :].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Run RFE on re-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set rfe args and combine with all other relevant args\n",
    "rfe_args = {'use_best_args': 1, 'study': study_name, 'rfe_ratio': 0.1, 'rfe_ksplits': 3, 'rfe_njobs': 1, 'rfe_scoring': 'average_precision'}\n",
    "tune_args = get_tune_args({**rfe_args, **vars(best_args)})\n",
    "rfe_combined_args = get_args({**tune_args})\n",
    "rfe_combined_args.split = 'dev/test' # performs k-fold on dev set\n",
    "\n",
    "# Run RFE\n",
    "rfe.main(rfe_combined_args, study=study_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Post-RFE model retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe_features = np.load(os.path.join(\"optuna_studies\", study_name, \"RFE_features.pkl\"), allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rfe(splits, rfe_features):\n",
    "    for i, split in enumerate(splits):\n",
    "        rfe_indcs = [feature_names.index(feature) for feature in rfe_features]\n",
    "        splits[i] = np.array(split)[:, rfe_indcs]\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_eval, y_eval, n_features, feature_names, class_weights = get_data(best_args) \n",
    "x_train, x_eval = get_rfe(x_train, rfe_features), get_rfe(x_eval, rfe_features)\n",
    "# retrain with best hyperparams\n",
    "_, models = start_training(best_args, None, 'auc', x_train, y_train, x_eval, y_eval, rfe_features, class_weights)\n",
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use test split\n",
    "test_args = best_args\n",
    "test_args.split = \"dev/test\"\n",
    "test_args.nf = 0\n",
    "_, _, x_test, y_test, _, feature_names, _ = get_data(test_args) \n",
    "x_test = get_rfe(x_test, rfe_features)\n",
    "\n",
    "# define output directory for eval results\n",
    "test_output_dir = os.path.join(\"../final_eval_results\", study_name, \"post-rfe\")\n",
    "os.makedirs(test_output_dir, exist_ok=True)\n",
    "\n",
    "# run evaluation (computs cumulative metrics if more than one model is evaluated here)\n",
    "evaluate_model(models, x_test, y_test, test_output_dir, rfe_features, test_args)\n",
    "\n",
    "print(f\"Find evaluation results in\\n {test_output_dir}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. After post-RFE retuning (done external to this script) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
